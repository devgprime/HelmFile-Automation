# Default values for flink-app.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

helm_scaffold_version: 0.4 # This is still only valid for the "default network policy" file
docker:
  pull_policy: IfNotPresent
elastic_log_namespace: nts_np_gulv
chartName: mni-ct-carrierkpi-4g-alarms
service:
  deployment: minikube

monitoring:
  enabled: true

app:
  image: gulv-space-rtt-docker-np.oneartifactoryci.verizon.com/build-images/rttflink
  version: 13d9f715
  podmonitor:
    matchnamespace: gulv-uat-aggregator
  flinkVersion: v1_18
  port: 8081
  job:
    jarURI: local:///opt/flink/usrlib/flinkaggregator-bundled-0.1.jar
    entryClass: "com.vzw.aggregator.Aggregator_v2"
    args: ["--inputJson", "/flink-json-configs/MNI/4G/mni-carrierkpis-4g-alarms.json"]
    parallelism: 24
    # Upgrade mode of the Flink job.
    # https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#stateful-and-stateless-application-upgrades
    upgradeMode: stateless

    # Desired state for the job., either 'running'' or 'suspended'
    #state: running
    # Nonce used to manually trigger savepoint for the running job.
    #savepointTriggerNonce: 1
    # Savepoint path used by the job the first time it is deployed.
    #initialSavepointPath:
    # Allow checkpoint state that cannot be mapped to any job vertex in tasks.
    #allowNonRestoredState: false

  # key/value pairs of flink-conf.yaml configuration.
  # NOTE: all values must be strings.
  flinkConfiguration:
    #kubernetes.cluster-id: mni-carrieragg4g-alarms
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
    high-availability.type: KUBERNETES
    high-availability.storageDir: /app/flink
    job.autoscaler.target.utilization: '0.6'
    job.autoscaler.target.utilization.boundary: '0.2'
    job.autoscaler.catch-up.duration: 5m
    job.autoscaler.restart.time: 2m
    job.autoscaler.scaling.enabled: 'true'
    job.autoscaler.stabilization.interval: 1m
    job.autoscaler.enabled: 'true'
    job.autoscaler.metrics.window: 3m
    taskmanager.numberOfTaskSlots: "4"
    heartbeat.timeout: "50000"
    taskmanager.memory.task.off-heap.size: "128m"
    taskmanager.memory.jvm-metaspace.size: "1024m"
    #taskmanager.memory.managed.size: "2gb"
    restart-strategy.type: exponential-delay
    restart-strategy.exponential-delay.backoff-multiplier: "2.0"
    restart-strategy.exponential-delay.initial-backoff: "1 s"
    restart-strategy.exponential-delay.jitter-factor: "0.1"
    restart-strategy.exponential-delay.max-backoff: "5 min"
    restart-strategy.exponential-delay.reset-backoff-threshold: "30 min"
  logConfiguration:
    rootLogger.level: INFO
    rootLogger.appenderRef.console.ref: LogConsole
    appender.console.name: LogConsole
    appender.console.type: CONSOLE
    appender.console.layout.type: PatternLayout
    appender.console.layout.pattern: '%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n'
    logger.akka.name: akka
    logger.akka.level: INFO
    logger.kafka.name: org.apache.kafka
    logger.kafka.level: INFO
    logger.hadoop.name: org.apache.hadoop
    logger.hadoop.level: INFO
    logger.zookeeper.name: org.apache.zookeeper
    logger.zookeeper.level: INFO
    logger.netty.name: org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level: OFF

  # List of environment variable to set in the flink containers.
  # These will be set for both Job and TaskManager containers.
  env:
  - name: MNI_KAFKA_ROLLUP_TOPIC_4G
    valueFrom:
      configMapKeyRef:
        name: aggregator-common
        key: MNI_CT_KAFKA_ROLLUP_TOPIC_4G

  - name: REGION
    valueFrom:
      configMapKeyRef:
        name: aggregator-common
        key: REGION_CT
  envfrom:
  - configMapRef:
      name: aggregator-common
  - secretRef:
      name: aggregator-common

  volumemount:
    - name: kafka-certs
      mountPath: /certs/
    - name: ha-directory
      mountPath: "/app/flink"
      readOnly: false
    - name: tmpfs
      mountPath: /tmp

  volumes:
  - name: kafka-certs
    secret:
      secretName: kafka-certs
      defaultMode: 420
  - name: ha-directory
    persistentVolumeClaim:
      claimName: gulv-aggregator-ha
  - name: tmpfs
    emptyDir: {}

  # JobManagerSpec
  # https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/#jobmanagerspec
  jobManager:
    replicas: 2
    resource:
      memory: 2048m
      cpu: 1

  # TaskManagerSpec
  # https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/#taskmanagerspec
  taskManager:
    resource:
      memory: 15G
      cpu: 2

  # TODO: log level configuration?

  # Set of config_files to deploy to jobmanager & taskmanager PODS
  # config_files:
  #   app_config:
  #     file_name: "app_config_config.yaml"
  #     content:
  #       entry1: value1
  config_files:
    # Can be used via eventutilities python with the --config arg (--config can be used multiple times)
    # or manually by loading it as a dict and passing it to logging.config.dictConfig
    # with the ecs-logging module installed.
    python.log.config.yaml:
      log_config:
        root:
          level: INFO
          handlers: [consoleHandler]

        handlers:
          consoleHandler:
            class: logging.StreamHandler
            formatter: ecsStdLibFormatter
            stream  : ext://sys.stdout

        formatters:
          ecsStdLibFormatter:
            class: ecs_logging.StdlibFormatter


config:
  public: {} # Add here all the keys that can be publicly available as a ConfigMap
  private: {} # Add here all the keys that should be private but still available as env variables



# Should be provided by configuration management.
# See details of the structures in the comments
# In the configuration module.
services_proxy: ~
tcp_services_proxy: ~

flink:
  object_store: ~
  # Enable HA using zookeeper. Requires object_store to be configured.
  high_availability: ~

# Add here the list of kafka-clusters (by name) that the service will need to reach.
kafka:
  allowed_clusters: []

# Add here the list of zookeeper-clusters (by name) that the service will need to reach.
zookeeper:
  allowed_clusters: []

# Allow external traffic to reach this service via a (cluster provided) ingress controller.
# https://wikitech.wikimedia.org/wiki/Kubernetes/Ingress#Configuration_(for_service_owners)
#
# TODO: In the future, we'd like to configure ingress to JobManager UI on the JobManager pods only.
#       See ideas for doing this here:
#       https://phabricator.wikimedia.org/T324576#8498319
#       For now, ingress is disabled.
#
ingress:
  enabled: true
  clustersuffix: apps.nsos-h4xv-cosp-dev3.nss.vzwnet.com

debug:
  enabled: false # Not supported